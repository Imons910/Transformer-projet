{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8798846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ›‘ Ã‰valuation du modÃ¨le AVANT Fine-Tuning (Zero-Shot)...\n",
      "ðŸ“Š Lancement : Zero-Shot Eval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Zero-Shot Eval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:26<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores initiaux : {'abs_rel': 1.2894021034240724, 'rmse': 1.794843610127767, 'log_rmse': 0.5735661745071411, 'delta': 0.16639939645926158}\n",
      "ðŸ”¥ DÃ©but du Fine-Tuning sur 10 Ã©poques...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:52<00:00,  7.47s/it, Delta=0.244, Loss=1.381]\n",
      "Ep 2/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:18<00:00,  5.21s/it, Delta=0.246, Loss=0.829]\n",
      "Ep 3/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:29<00:00,  5.99s/it, Delta=0.224, Loss=0.830]\n",
      "Ep 4/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:19<00:00,  5.28s/it, Delta=0.493, Loss=0.227]\n",
      "Ep 5/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:25<00:00,  5.71s/it, Delta=0.418, Loss=0.253]\n",
      "Ep 6/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:18<00:00,  5.22s/it, Delta=0.517, Loss=0.187]\n",
      "Ep 7/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:51<00:00,  7.44s/it, Delta=0.553, Loss=0.176]\n",
      "Ep 8/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:34<00:00,  6.27s/it, Delta=0.697, Loss=0.083]\n",
      "Ep 9/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [02:22<00:00,  9.53s/it, Delta=0.686, Loss=0.117]\n",
      "Ep 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:43<00:00,  6.91s/it, Delta=0.883, Loss=0.109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "ðŸ“ˆ BILAN AVANT / APRÃˆS\n",
      "AVANT (Zero-Shot) -> Delta: 0.1664 | RMSE: 1.7948\n",
      "APRÃˆS (Fine-Tuned)-> Delta: 0.7526 | RMSE: 0.3187\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "FORCE_CPU = True # Mettre True pour test local\n",
    "MODEL_ID = \"depth-anything/Depth-Anything-V2-Small-hf\"\n",
    "OUTPUT_DIR = \"./resultats_projet_v2\"\n",
    "BATCH_SIZE = 4\n",
    "LR = 1e-4\n",
    "EPOCHS = 10\n",
    "\n",
    "if FORCE_CPU:\n",
    "    DEVICE = \"cpu\"\n",
    "else:\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ================= DATASET =================\n",
    "class ZividDataset(Dataset):\n",
    "    def __init__(self, root_dir, processor):\n",
    "        self.img_dir = os.path.join(root_dir, \"images\")\n",
    "        self.depth_dir = os.path.join(root_dir, \"depth\")\n",
    "        self.processor = processor\n",
    "        valid_ext = ('.png', '.jpg', '.jpeg')\n",
    "        if not os.path.exists(self.img_dir): raise FileNotFoundError(f\"âŒ Dossier introuvable : {self.img_dir}\")\n",
    "        self.images = sorted([f for f in os.listdir(self.img_dir) if f.lower().endswith(valid_ext)])\n",
    "\n",
    "    def __len__(self): return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        base_name = os.path.splitext(img_name)[0].replace(\"_color\", \"\")\n",
    "        npy_name = base_name + \"_rawDepth.npy\"\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        npy_path = os.path.join(self.depth_dir, npy_name)\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        try:\n",
    "            point_cloud = np.load(npy_path)\n",
    "            depth_Z = point_cloud[:, :, 2]\n",
    "            # Conversion mm -> mÃ¨tres\n",
    "            if np.nanmax(depth_Z) > 100: depth_Z = depth_Z / 1000.0\n",
    "        except: return None\n",
    "\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        target_h, target_w = inputs[\"pixel_values\"].shape[-2:]\n",
    "        depth_tensor = torch.from_numpy(depth_Z).float().unsqueeze(0).unsqueeze(0)\n",
    "        mask = ~torch.isnan(depth_tensor) & ~torch.isinf(depth_tensor) & (depth_tensor > 0)\n",
    "        depth_tensor = torch.nan_to_num(depth_tensor, nan=0.0)\n",
    "        \n",
    "        depth_resized = torch.nn.functional.interpolate(depth_tensor, size=(target_h, target_w), mode='nearest')\n",
    "        mask_resized = torch.nn.functional.interpolate(mask.float(), size=(target_h, target_w), mode='nearest')\n",
    "        \n",
    "        return {\"pixel_values\": inputs[\"pixel_values\"].squeeze(0), \"labels\": depth_resized.squeeze(0), \"mask\": mask_resized.squeeze(0)}\n",
    "\n",
    "# ================= MÃ‰TRIQUES =================\n",
    "def compute_metrics(pred, target, mask):\n",
    "    pred = pred[mask]\n",
    "    target = target[mask]\n",
    "    if len(target) == 0: return 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    # 1. AbsRel\n",
    "    abs_rel = torch.mean(torch.abs(pred - target) / target)\n",
    "    # 2. RMSE\n",
    "    rmse = torch.sqrt(torch.mean((pred - target) ** 2))\n",
    "    # 3. Log RMSE (Nouveau !)\n",
    "    log_rmse = torch.sqrt(torch.mean((torch.log1p(pred) - torch.log1p(target)) ** 2))\n",
    "    # 4. Delta < 1.25\n",
    "    max_ratio = torch.max(pred / target, target / pred)\n",
    "    delta1 = (max_ratio < 1.25).float().mean()\n",
    "\n",
    "    return abs_rel.item(), rmse.item(), log_rmse.item(), delta1.item()\n",
    "\n",
    "# ================= VISUALISATION =================\n",
    "def save_curves(history):\n",
    "    epochs = range(1, len(history['loss']) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Courbe Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history['loss'], 'b-o', label='Training Loss')\n",
    "    plt.title(\"Ã‰volution de la Loss (MSE)\")\n",
    "    plt.xlabel(\"Ã‰poques\"); plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Courbe Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history['delta'], 'g-o', label='Accuracy (Delta < 1.25)')\n",
    "    plt.title(\"Ã‰volution de la PrÃ©cision\")\n",
    "    plt.xlabel(\"Ã‰poques\"); plt.ylabel(\"Delta %\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, \"courbes_entrainement.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_full_dataset(model, loader, desc=\"Evaluation\"):\n",
    "    \"\"\"Ã‰value le modÃ¨le sur tout le dataset sans entraÃ®nement\"\"\"\n",
    "    model.eval()\n",
    "    total_metrics = {'abs_rel': 0, 'rmse': 0, 'log_rmse': 0, 'delta': 0}\n",
    "    count = 0\n",
    "    \n",
    "    print(f\"ðŸ“Š Lancement : {desc}...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=desc):\n",
    "            pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "            mask = batch[\"mask\"].to(DEVICE)\n",
    "            \n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            prediction = torch.nn.functional.interpolate(\n",
    "                outputs.predicted_depth.unsqueeze(1), size=labels.shape[-2:], mode=\"bilinear\", align_corners=False\n",
    "            )\n",
    "            \n",
    "            ar, rm, lrm, d1 = compute_metrics(prediction, labels, mask.bool())\n",
    "            total_metrics['abs_rel'] += ar\n",
    "            total_metrics['rmse'] += rm\n",
    "            total_metrics['log_rmse'] += lrm\n",
    "            total_metrics['delta'] += d1\n",
    "            count += 1\n",
    "            \n",
    "    return {k: v/count for k, v in total_metrics.items()}\n",
    "\n",
    "# ================= MAIN =================\n",
    "def run_project():\n",
    "    base_dir = os.getcwd()\n",
    "    dataset_dir = os.path.join(base_dir, \"DATASET_DEVOIR\")\n",
    "    \n",
    "    processor = AutoImageProcessor.from_pretrained(MODEL_ID)\n",
    "    model = AutoModelForDepthEstimation.from_pretrained(MODEL_ID)\n",
    "    \n",
    "    # --- 1. CONFIG LoRA ---\n",
    "    lora_config = LoraConfig(r=16, lora_alpha=16, target_modules=[\"query\", \"value\"], lora_dropout=0.1, bias=\"none\")\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # --- 2. DATASET ---\n",
    "    try:\n",
    "        dataset = ZividDataset(dataset_dir, processor)\n",
    "        dataset = [d for d in dataset if d is not None]\n",
    "    except Exception as e: print(e); return\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # --- 3. EVALUATION AVANT ENTRAINEMENT (ZERO-SHOT) ---\n",
    "    print(\"ðŸ›‘ Ã‰valuation du modÃ¨le AVANT Fine-Tuning (Zero-Shot)...\")\n",
    "    metrics_before = evaluate_full_dataset(model, loader, desc=\"Zero-Shot Eval\")\n",
    "    print(f\"Scores initiaux : {metrics_before}\")\n",
    "\n",
    "    # --- 4. ENTRAINEMENT ---\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    history = {'loss': [], 'delta': []}\n",
    "    \n",
    "    model.train()\n",
    "    print(f\"ðŸ”¥ DÃ©but du Fine-Tuning sur {EPOCHS} Ã©poques...\")\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        epoch_loss = 0\n",
    "        epoch_delta = 0\n",
    "        loop = tqdm(loader, desc=f\"Ep {epoch}/{EPOCHS}\")\n",
    "        \n",
    "        for batch in loop:\n",
    "            pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "            mask = batch[\"mask\"].to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            prediction = torch.nn.functional.interpolate(outputs.predicted_depth.unsqueeze(1), size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "            \n",
    "            loss = torch.sum(mask * (prediction - labels)**2) / (torch.sum(mask) + 1e-6)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                _, _, _, d1 = compute_metrics(prediction, labels, mask.bool())\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_delta += d1\n",
    "            loop.set_postfix(Loss=f\"{loss.item():.3f}\", Delta=f\"{d1:.3f}\")\n",
    "        \n",
    "        avg_loss = epoch_loss/len(loader)\n",
    "        avg_delta = epoch_delta/len(loader)\n",
    "        history['loss'].append(avg_loss)\n",
    "        history['delta'].append(avg_delta)\n",
    "        \n",
    "    # --- 5. RESULTATS & SAUVEGARDE ---\n",
    "    model.save_pretrained(os.path.join(OUTPUT_DIR, \"modele_final_lora\"))\n",
    "    save_curves(history) # GÃ©nÃ¨re l'image des courbes\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"ðŸ“ˆ BILAN AVANT / APRÃˆS\")\n",
    "    print(f\"AVANT (Zero-Shot) -> Delta: {metrics_before['delta']:.4f} | RMSE: {metrics_before['rmse']:.4f}\")\n",
    "    print(f\"APRÃˆS (Fine-Tuned)-> Delta: {history['delta'][-1]:.4f} | RMSE: {np.sqrt(history['loss'][-1]):.4f}\") # Approx RMSE from loss\n",
    "    print(\"=\"*30)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_project()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_projet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
